{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seattle Airbnb Open Data Analysis\n",
    "#### Business Problem: Drive booking rate for groups of hosts whose listings have exceptionally low monthly booking rate.\n",
    "#### Solution: Identify well-performing hosts and contributing attributes of this poor performing group that lead to significant improvements in their listings' monthly booking rates, recommend poor to mediocre hosts of this subgroup to carry out actions to obtain such attributes in their listings.\n",
    "\n",
    "#### Exploratory Data Analysis: \n",
    "Hosts are grouped by have consistently seen high duration difference between the date they joined Airbnb and the last scraped date of their resepective listings, hosts are binned into years of 1 with the exception of >= 5 years.   (booking rates for all months are above median, belong to q3 and q4: upper half amongst listings)\n",
    "1. Identify the group of hosts that have the lowest monthly booking rate\n",
    "2. Perform further faceted analysis on this subgroup\n",
    "3. Feature engineering on listings' features (deriving binary variables for presence of transit text, extracting sentiment scores of text variables)\n",
    "4. Identify listings from the lowest monthly booking rate group that have consistently seen low monthly booking rates (booking rates for all months are below or equal to median, belong to q1 and q2: lower half amongst listings)\n",
    "5. Identify listings from the lowest monthly booking rate group that have consistently seen high monthly booking rates, these hosts are deemed as the top performing hosts of the subgroup (booking rates for all months are above median, belong to q3 and q4: upper half amongst listings)\n",
    "6. Perform comparative analysis on these two subgroups (from fourth step and fifth step) to identify their key differences on existing features and engineered features\n",
    "\n",
    "#### Classification: \n",
    "#### Given set of attributes of a listing, predict the listing's booking rate quartile performance for the month (Q1, Q2, Q3 or Q4). \n",
    "#### If it is lower than the median booking rate (Q1 and Q2), remedy steps will be provided to the host to improve on his/her listing.\n",
    "1. Select all relevant attributes\n",
    "2. Perform encoding of categorical features and standardisation of numerical features\n",
    "2. Feature selection using Lasso\n",
    "3. Train-val-test split\n",
    "4. Modeling\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Imports, loading data, basic checks and simple data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaValueError: too few arguments, must supply command line package specs or --file\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-be8e0c27d228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLassoCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "!conda install \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "import afinn\n",
    "from afinn import Afinn\n",
    "\n",
    "# For POS and NER tag counting\n",
    "from collections import Counter\n",
    "\n",
    "# Preprocessing, modeling libraries\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib inline visualisation display and default figure size\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (25.0, 15.0)\n",
    "RANDOM_SEED = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('data/calendar.csv')\n",
    "listings = pd.read_csv('data/listings.csv')\n",
    "reviews = pd.read_csv('data/reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic scanning of datasets to check for adnormalities and null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of nulls\n",
    "print('Number of nulls\\n', calendar.isna().sum(), '\\n')\n",
    "# check percentage of nulls\n",
    "print('Percentage of nulls\\n', calendar.isna().sum()/calendar.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of nulls\n",
    "print('Number of nulls\\n', listings.isna().sum(), '\\n')\n",
    "# check percentage of nulls\n",
    "print('\\nPercentage of nulls\\n', listings.isna().sum()/listings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of nulls\n",
    "print('Number of nulls\\n', reviews.isna().sum(), '\\n')\n",
    "# check percentage of nulls\n",
    "print('\\nPercentage of nulls\\n', reviews.isna().sum()/reviews.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of date attributes for datasets and removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_numerical = ['host_listings_count', 'host_total_listings_count', 'accommodates','bathrooms', \n",
    "                      'bedrooms', 'beds', 'guests_included', 'minimum_nights','maximum_nights', \n",
    "                      'availability_30','availability_60', 'availability_90', 'availability_365',\n",
    "                      'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', \n",
    "                      'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                      'calculated_host_listings_count', 'reviews_per_month']\n",
    "\n",
    "listings_currency = ['price', 'weekly_price', 'monthly_price', 'security_deposit', \n",
    "                     'cleaning_fee','extra_people']\n",
    "\n",
    "# fill na with 'N/A' strings\n",
    "fill_na_strings_cols = ['host_response_time', 'host_response_rate', 'host_acceptance_rate']\n",
    "\n",
    "# fill na with 1\n",
    "fill_na_1_cols = ['bathrooms', 'bedrooms', 'beds']\n",
    "\n",
    "# fill na with 0\n",
    "fill_na_0_cols = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', \n",
    "                  'review_scores_value', 'security_deposit', 'cleaning_fee']\n",
    "\n",
    "convert_binary_features_url = ['thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url']\n",
    "\n",
    "convert_binary_features_tf = ['host_has_profile_pic', 'host_identity_verified', 'is_location_exact', \n",
    "                              'instant_bookable', 'require_guest_profile_picture',\n",
    "                              'require_guest_phone_verification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all dates to pandas datetime64\n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "listings['last_scraped'] = pd.to_datetime(listings['last_scraped'])\n",
    "listings['host_since'] = pd.to_datetime(listings['host_since'])\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])\n",
    "\n",
    "# set all values to float\n",
    "listings[listings_numerical] = listings[listings_numerical].astype(float, copy=False)\n",
    "\n",
    "# set currency values to float\n",
    "listings[listings_currency] = listings[listings_currency].replace('[\\$,]', '', regex=True).astype(float, copy=False)\n",
    "\n",
    "listings[fill_na_strings_cols] = listings[fill_na_strings_cols].fillna('N/A')\n",
    "listings[fill_na_1_cols] = listings[fill_na_1_cols].fillna(1)\n",
    "listings[fill_na_0_cols] = listings[fill_na_0_cols].fillna(0)\n",
    "listings['property_type'] = listings['property_type'].fillna('House')\n",
    "\n",
    "# convert features to binary\n",
    "for feature in convert_binary_features_url:\n",
    "    listings[f'{feature}_exist'] = listings[feature].apply(lambda url: 1 if pd.notna(url) else 0)\n",
    "\n",
    "for feature in convert_binary_features_tf:\n",
    "    listings[f'{feature}_1'] = listings[feature].apply(lambda boolean: 1 if boolean == 't' else 0)\n",
    "\n",
    "# drop duplicates for all 3 datasets\n",
    "calendar.drop_duplicates(keep='first', inplace=True)\n",
    "listings.drop_duplicates(keep='first', inplace=True)\n",
    "reviews.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Monthly Booking Rate variation of different listings faceted by distinct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There is {listings.last_scraped.nunique()} unique value(s) for variable last_scraped in the listings dataset, the unique value(s) is/are: {listings.last_scraped.unique()[0].astype(str)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting out month and year for grouping by\n",
    "calendar['year'] = calendar['date'].apply(lambda date: date.year)\n",
    "calendar['month'] = calendar['date'].apply(lambda date: date.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by listing id, year and month and availability\n",
    "monthly_availability = calendar.drop(['price'], axis=1).groupby(['listing_id', 'year', 'month', 'available']).count().rename({'date': 'availability_days'}, axis=1)\n",
    "monthly_availability.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain percentage of availability by listing id, year and month\n",
    "monthly_availability_percent = monthly_availability.groupby(level=[0, 1, 2]).apply(lambda g: \n",
    "                                                                                   g / g.sum()).rename({'availability_days': 'availability_percentage'}, axis=1).unstack(level=-1)\n",
    "monthly_availability_percent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain monthly booking rate for listings\n",
    "monthly_booking_rate = pd.DataFrame(monthly_availability_percent.apply(lambda row: row.availability_percentage.f \n",
    "                                                                       if not np.isnan(row.availability_percentage.f) \n",
    "                                                                       else 1-row.availability_percentage.t, axis=1)).stack().reset_index().drop('level_3', axis=1).rename({0: 'booking_rate'}, axis=1)\n",
    "monthly_booking_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3)\n",
    "colours = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:grey', 'tab:olive', 'tab:cyan', 'gold', 'mediumspringgreen']\n",
    "\n",
    "for index, colour in enumerate(colours):\n",
    "    sns.distplot(monthly_booking_rate[monthly_booking_rate['month'] == index+1]['booking_rate'], ax=axes[int(index/3),int(index%3)], color=colour, axlabel='', kde=True).set_title(f'Dist plot for month {index+1}')\n",
    "\n",
    "fig.text(0.5, 0.04, 'Monthly booking rate', ha='center', fontsize=25)\n",
    "fig.text(0.04, 0.5, 'Gaussian kernel density estimate', va='center', rotation='vertical', fontsize=25)\n",
    "fig.suptitle('Distribution plots of Monthly Booking Rates by Months', fontsize=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "Highly similiar bi-modal distribution observed for all distribution plots. This is an indication that majority of the listings either have zero or near zero booking rate or have nearly maximum booking rate at 1.0. It is only observed in the earlier months of the year (January and February) that there are a noticeable portion of listings spreading between 0.0 and 1.0 monthly booking rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple helper function for EDA with option for trimmed mean to account for outliers\n",
    "# e.g. trim_proportion = 0.05 means trim values that are less and equal to 5th percentile value and values that are greater or equal to 95th percentile value\n",
    "# default to trim 0.05\n",
    "def booking_rate_filter(monthly_booking_rate_df, listings_df, listings_filter_var, average_months=True, trim_proportion=0.05, joined_df_only=False):\n",
    "    if monthly_booking_rate_df is not None:\n",
    "        filtered_listings_df = listings_df[['id', listings_filter_var]]\n",
    "        joined_df = monthly_booking_rate_df.join(filtered_listings_df.set_index('id'), on='listing_id', how='inner')\n",
    "    else:\n",
    "        joined_df = listings_df\n",
    "    if joined_df_only:\n",
    "        return_df = joined_df\n",
    "    else:\n",
    "        if average_months:\n",
    "            if trim_proportion:\n",
    "                lower_quantile = joined_df['booking_rate'].quantile(trim_proportion)\n",
    "                upper_quantile = joined_df['booking_rate'].quantile(1-trim_proportion)\n",
    "                joined_df = joined_df[(joined_df['booking_rate'] > lower_quantile) & (joined_df['booking_rate'] < upper_quantile)]\n",
    "            return_df = joined_df.groupby([listings_filter_var]).mean().drop(['listing_id', 'year', 'month'], axis=1).rename({'booking_rate': 'average_monthly_booking_rate'}, axis=1)\n",
    "        else:\n",
    "            if trim_proportion:\n",
    "                lower_quantile = joined_df.groupby(['month']).quantile(trim_proportion, axis=0)['booking_rate']\n",
    "                upper_quantile = joined_df.groupby(['month']).quantile(1-trim_proportion, axis=0)['booking_rate']\n",
    "                joined_df['lower_quantile'] = joined_df.apply(lambda row: lower_quantile[row.month], axis=1)\n",
    "                joined_df['upper_quantile'] = joined_df.apply(lambda row: upper_quantile[row.month], axis=1)\n",
    "                joined_df = joined_df[(joined_df.booking_rate > joined_df['lower_quantile']) & (joined_df.booking_rate < joined_df['upper_quantile'])].drop(['lower_quantile', 'upper_quantile'], axis=1)\n",
    "            return_df = joined_df.groupby([listings_filter_var, 'month']).mean().drop(['listing_id', 'year'], axis=1).rename({'booking_rate': 'monthly_booking_rate'}, axis=1)\n",
    "    return return_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation to bin years of hosting on airbnb (host_since) into distinct categories\n",
    "\n",
    "# helper function for binning datetime\n",
    "one_year = pd.Timedelta('365 days')\n",
    "def airbnb_age(listings_row):\n",
    "    if listings_row.host_since == pd.NaT or listings_row.last_scraped == pd.NaT:\n",
    "        val = 'Missing one datetime value'\n",
    "    else:\n",
    "        airbnb_age_days = listings_row.last_scraped - listings_row.host_since\n",
    "        if airbnb_age_days < one_year:\n",
    "            val = '< 1 year'\n",
    "        elif airbnb_age_days < one_year*2:\n",
    "            val = '1 year <= & < 2 years'\n",
    "        elif airbnb_age_days < one_year*3:\n",
    "            val = '2 years <= & < 3 years'\n",
    "        elif airbnb_age_days < one_year*4:\n",
    "            val = '3 years <= & < 4 years'\n",
    "        elif airbnb_age_days < one_year*5:\n",
    "            val = '4 years <= & < 5 years'\n",
    "        else:\n",
    "            val = '>= 5 years'\n",
    "    return val\n",
    "\n",
    "# create column\n",
    "listings['airbnb_age'] = listings.apply(airbnb_age, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Monthly Booking Rate by Host Airbnb Age  (Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_rate_filter(monthly_booking_rate, listings, 'airbnb_age', average_months=False, trim_proportion=None).unstack(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: <br>\n",
    "\n",
    "As observed from the above visualisation, the monthly booking rate for hosts by their airbnb age is shown. <br>\n",
    "\n",
    "Hosts with 4 years or more but less than 5 years of hosting experience have the lowest monthly booking rate amongst all age groups and for all months. The lowest monthly booking rate can be observed in December as this host group have a monthly booking rate of approximately 17% with the next lowest group (host with 3 years but less than 4 yeasr hosting experience) being at least 25% higher in monthly booking rate.<br>\n",
    "\n",
    "Hosts with less than 1 year of hosting experience are observed to have the highest monthly booking rate as it is likely attributed from customers attraction to the novelty of new hosts' listings. Furthermore, it is an indicator that customers do steer from hosts with lesser experience on Airbnb. <br>\n",
    "\n",
    "However, this is calculated using arithmetic mean and is prone to be influenced heavily to outliers, therefore the next visualisation will be showcasing trimmed mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Monthly Booking Rate by Host Airbnb Age  (Trimmed Mean, Trimmed 5th and 95th percentile values and lesser/greater respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_rate_filter(monthly_booking_rate, listings, 'airbnb_age', average_months=False).unstack(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: <br>\n",
    "\n",
    "Stark constrasts can be observed by comparing the above visualisation to the visualisation with arithmetic mean. Hosts with 3-4 years hosting experience have became the worst performing hosts by trimmed mean monthly booking rate for the months 8, 11, 12 (August, Novemeber and December). <br>\n",
    "\n",
    "By excluding fully booked and fully unbooked listings for the months, it can be observed that the months 4 (April) and 12 (December) are the peak periods for mediocre performing listings as the average amongst hosts are the highest in respect to the months. <br>\n",
    "\n",
    "**As hosts with 4-5 years hosting experience (showed in the chart as red bars) are still performing relatively weak as observed from the majority of the months being the last or second-to-last in terms of monthly booking rate performance. Therefore, it is chosen as a group to conduct further analysis on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter listings to listings of hosts where hosting experience [4 years, 5 years)\n",
    "four_years_host_listings = listings[listings['airbnb_age'] == '4 years <= & < 5 years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of hosts in this group\n",
    "print(f'Number of unique hosts in this group is {four_years_host_listings.host_id.nunique()}')\n",
    "\n",
    "# Check number of listings in this group\n",
    "print(f'Number of unique listings in this group is {four_years_host_listings.id.nunique()}')\n",
    "\n",
    "# Check mean listings/host\n",
    "print(f'Average (Arithmetic Mean) number of listings per host is {four_years_host_listings.id.nunique()/four_years_host_listings.host_id.nunique():.2f}')\n",
    "\n",
    "# Check trimmed mean listings/host\n",
    "num_listings = four_years_host_listings.groupby(['host_id']).count()['id']\n",
    "print(f'Average (Trimmed Mean, 0.05 Cut) number of listings per host is {trim_mean(num_listings, 0.05):.2f}')\n",
    "\n",
    "# Check median listings/host\n",
    "print(f'Average (Median) number of listings per host is {num_listings.median()}')\n",
    "\n",
    "# Check mode listings/host\n",
    "print(f'Average (Mode) number of listings per host is {float(num_listings.mode()[0])}')\n",
    "\n",
    "# Simple distribution plot of listings/host\n",
    "ax = sns.distplot(num_listings, color='salmon')\n",
    "ax.set_title('Distribution plot of host listings', fontsize=25)\n",
    "ax.set_ylabel('Percentage', fontsize=15)\n",
    "ax.set_xlabel('Number of listings for hosts', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "Using the averages and the distribution plot for analysis, it is clear that number of listings for hosts is a right skewed distribution with majority of hosts having 1 to 3 listings. The distribution quickly tails off with only a small noticeable proportion of hosts having 4 to 10 listings. Outliers can be observed from 15 listings and above with a significantly sized cluster at around 35 listings. These hosts are likely private firms that manage rental property and are using airbnb for all the properties that they manage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Monthly Booking Rate by Superhost status for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_rate_filter(monthly_booking_rate, four_years_host_listings, 'host_is_superhost', average_months=False, trim_proportion=0).unstack(0).plot(kind='bar', color=['teal', 'orange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: <br><br>\n",
    "As expected, monthly booking rates for superhosts are significantly higher than those who are not with the exception of 3 months (February, March and April). This reinforces the intuition that superhost status plays a part in attaining higher booking rates for hosts of this group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Heat Map of booking rate facted by verified host identify for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(booking_rate_filter(monthly_booking_rate, four_years_host_listings, 'host_identity_verified', average_months=False, trim_proportion=0).unstack(0), annot=True, fmt=\"2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: <br>\n",
    "    \n",
    "There is a cluster of months (Januaray to March) where unverified hosts have higher booking rates than verified hosts. However, other months such as 5, 6, 7 (May, June, July) have shown higher booking rates for verified hosts than unverified hosts. Henceforth, no conclusive statements can be made on whether host verification has a signficant impact on booking rate and should be deemed as insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Donut charts of listing categorical variables for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vars = ['is_location_exact', 'room_type', 'instant_bookable', 'cancellation_policy']\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "colours = ['peachpuff', 'lightpink', 'paleturquoise', 'lightgreen']\n",
    "\n",
    "for index, var in enumerate(plot_vars):\n",
    "    test = four_years_host_listings[var].value_counts()\n",
    "    axes[int(index/2), int(index%2)].pie(test, labels=test.keys(), autopct='%1.2f%%', shadow=True, colors=colours)\n",
    "    \n",
    "    axes[int(index/2), int(index%2)].set_title(f'Donut plot for variable {var}')\n",
    "    \n",
    "    my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "    axes[int(index/2), int(index%2)].add_artist(my_circle)\n",
    "\n",
    "fig.suptitle('Donut charts of 4-5 years hosts faceted by categorical variables', fontsize=25, va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "Top-left donut plot: A large majority of 4-5 years hosts prefer to have their listings' location to be set as exact instead of set as an area. This indicates that they do not see any security repercussions and may be convinced that it will aid their potential customers in their accommodation selection.<br>\n",
    "\n",
    "Top-right donut plot: Over 90% of listings either involves the rental of the entire apartment or a private room with almost 70% of the total coming from entire home/apartment rental. Only 9% of listings are of shared room accomodation which is not surprising as hostels provide fierce and direct competition against such listings.<br>\n",
    "\n",
    "Bottom-left donut plot: Over 90% of listings are not instant bookable by nature which may be evidence that hosts do not see the benefits of making their listings instant bookable.\n",
    "\n",
    "Bottom-right donut plot: Majority of the listings have strict cancellation policy (~58%) with only less than one-fifth (17.79%) having flexible cancellation policies. This will explored further on whether there is an impact on booking rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Monthly Booking Rate for hosts with at least 4 years but less than 5 years hosting experience by listing's cancellation policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = booking_rate_filter(monthly_booking_rate, four_years_host_listings, 'cancellation_policy', average_months=False, trim_proportion=0).unstack(0).plot(kind='barh', color=['powderblue', 'lemonchiffon', 'lightcoral'])\n",
    "ax.set_xlabel(\"Booking rate\", fontsize=20)\n",
    "ax.set_ylabel('Month', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "To little surprise, strict cancellation policy listings perform the worst in terms of booking rates for almost all months. The only exception that it is not the worst performing group of listings is in the month of January (1) when flexible cancellation policy listings performed worse.<br>\n",
    "\n",
    "Another interesting insight is that flexible cancellation policy listings did not performed the best for all the months in a year, topping the group in only for the months 2, 3, 4 (February, March and April).<br>\n",
    "\n",
    "Moderate cancellation policy listings has the best booking rate performance as it led for 9 out of 12 months. Its relative performance to flexible cancellation policy listings can be reasoned as customers are less likely to cancel moderate cancellation listings bookings than flexible cancellation policy bookings due to the higher penalty imposed by moderate cancellation policy listings over flexible cancellation policy listings. <br>\n",
    "\n",
    "Therefore, moderate cancellation policies appear to be the best in terms of maximising booking rate for listings in this group of hosts with 4-5 years hosting experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_text_features(text_series):\n",
    "    nlp = spacy.load('en')\n",
    "    afinn_emo = Afinn(emoticons=True)\n",
    "    return_dict = {'sentiment': None, 'pos': {}, 'ner': {}}\n",
    "    \n",
    "    # obtain sentiment scores using Afinn \n",
    "    return_dict['sentiment'] = text_series.apply(lambda text: afinn_emo.score(text) if not pd.isnull(text) else 0.0)\n",
    "    \n",
    "    # tokenise, remove stopwords and lemmatisation\n",
    "    prepped_text_series = text_series.apply(lambda text: ' '.join([token.lemma_ for token in nlp(text) if token.is_stop != True and token.is_punct != True]) if not pd.isnull(text) else '')\n",
    "\n",
    "    # pos tags counting\n",
    "    def pos_counter(prepped_texts):\n",
    "        prepped_texts_list = prepped_texts.apply(lambda prepped_text: [token.pos_ for token in nlp(prepped_text)]).tolist()\n",
    "        adj, adv, intj, noun, pron, propn, verb = [], [], [], [], [], [], []\n",
    "        for prepped_text in prepped_texts_list:\n",
    "            pos_freq = Counter(prepped_text)\n",
    "            adj.append(pos_freq['ADJ'])\n",
    "            adv.append(pos_freq['ADV'])\n",
    "            intj.append(pos_freq['INTJ'])\n",
    "            noun.append(pos_freq['NOUN'])\n",
    "            pron.append(pos_freq ['PRON'])\n",
    "            propn.append(pos_freq['PROPN'])\n",
    "            verb.append(pos_freq['VERB'])\n",
    "        return adj, adv, intj, noun, pron, propn, verb\n",
    "    \n",
    "    pos = {}\n",
    "    pos['adj'], pos['adv'], pos['intj'], pos['noun'], pos['pron'], pos['propn'], pos['verb'] = pos_counter(prepped_text_series)\n",
    "    \n",
    "    # named entity recognition counting\n",
    "    def ner_counter(unprepped_texts):\n",
    "        texts_ent_labels_list = unprepped_texts.apply(lambda text: [ent.label_ for ent in nlp(text).ents]  if not pd.isnull(text) else []).tolist()\n",
    "        person, norp, fac, gpe, loc, product, money, event, work_of_art = [], [], [], [], [], [], [], [], []\n",
    "        for text_ent_labels in texts_ent_labels_list:\n",
    "            ner_freq = Counter(text_ent_labels)\n",
    "            person.append(ner_freq['PERSON'])\n",
    "            norp.append(ner_freq['NORP'])\n",
    "            fac.append(ner_freq['FAC'])\n",
    "            gpe.append(ner_freq['GPE'])\n",
    "            loc.append(ner_freq['LOC'])\n",
    "            product.append(ner_freq['PRODUCT'])\n",
    "            money.append(ner_freq['MONEY'])\n",
    "            event.append(ner_freq['EVENT'])\n",
    "            work_of_art.append(ner_freq['WORK_OF_ART'])\n",
    "        return person, norp, fac, gpe, loc, product, money, event, work_of_art\n",
    "    \n",
    "    ner = {}\n",
    "    ner['person'], ner['norp'], ner['fac'], ner['gpe'], ner['loc'], ner['product'], ner['money'], ner['event'], ner['work_of_art'] = ner_counter(text_series)\n",
    "    \n",
    "    return_dict['pos'] = pos\n",
    "    return_dict['ner'] = ner\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for part-of-speech tag counts, named entity recognition label counts and sentiment scores for textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vars = ['name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'host_about']\n",
    "for text_var in text_vars:\n",
    "    return_dict = engineer_text_features(listings[text_var])\n",
    "    listings[f'{text_var}_sentiment'] = return_dict['sentiment']\n",
    "    for pos_tag, pos_tag_count in return_dict['pos'].items():\n",
    "        listings[f'{text_var}_{pos_tag}'] = pos_tag_count\n",
    "    for ner_tag, ner_tag_count in return_dict['ner'].items():\n",
    "        listings[f'{text_var}_{ner_tag}'] = ner_tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new variable: Quartile that monthly booking rates of listings belong to in respect to all listings' monthly booking rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def booking_rate_quartile(booking_rate_df, listings_df, by='airbnb_age'):\n",
    "    years = booking_rate_df['year'].unique()\n",
    "    return_df = booking_rate_df.set_index('listing_id').join(listings_df.set_index('id'), how='left').set_index(['month', 'year'], append=True)\n",
    "    return_df['booking_rate_quartile'] = 0\n",
    "    loop_df = return_df.copy(deep=True)\n",
    "    unique_column_values = return_df[by].unique()\n",
    "    for unique_value in unique_column_values:\n",
    "        for month in range(1,13):\n",
    "            for year in years:\n",
    "                unique_value_df = loop_df[loop_df[by] == unique_value]\n",
    "                unique_value_df = unique_value_df.loc[pd.IndexSlice[:, month, year], :]\n",
    "                booking_rates_series = unique_value_df['booking_rate']\n",
    "                q1_value, q2_value, q3_value = booking_rates_series.quantile(0.25), booking_rates_series.quantile(0.5), booking_rates_series.quantile(0.75)\n",
    "                unique_value_df['booking_rate_quartile'] = unique_value_df['booking_rate'].apply(lambda booking_rate: \n",
    "                                                                                                 1 if booking_rate <= q1_value else \n",
    "                                                                                                 (2 if booking_rate <= q2_value else \n",
    "                                                                                                  (3 if booking_rate <= q3_value else 4)))\n",
    "                return_df.update(unique_value_df['booking_rate_quartile'], join='left')\n",
    "    return_df['booking_rate_quartile'] = return_df['booking_rate_quartile'].apply(lambda quartile: int(quartile))\n",
    "    return return_df.reset_index().rename({'level_0': 'listing_id'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save updated dataframe with engineered features to save on computational time for further computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    listings_booking_rate_quartile = booking_rate_quartile(monthly_booking_rate, listings)\n",
    "except NameError:\n",
    "    monthly_booking_rate = pd.read_csv('monthly_booking_rate.csv')\n",
    "    listings = pd.read_csv('engineered_listings_before_booking_rate_quartile.csv')\n",
    "    listings_booking_rate_quartile = booking_rate_quartile(monthly_booking_rate, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_booking_rate_quartile.to_csv('listings_booking_rate_quartile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Further analysis on 4-5 years hosts' listings with engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace existing four_years_host_listings with new df with new features\n",
    "try:\n",
    "    four_years_host_listings = listings_booking_rate_quartile[listings_booking_rate_quartile['airbnb_age'] == '4 years <= & < 5 years']\n",
    "except NameError:\n",
    "    listings_booking_rate_quartile = pd.read_csv('listings_booking_rate_quartile.csv')\n",
    "    four_years_host_listings = listings_booking_rate_quartile[listings_booking_rate_quartile['airbnb_age'] == '4 years <= & < 5 years']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising engineered features distribution against aggregated booking rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:grey', 'tab:olive', 'tab:cyan', 'gold', 'mediumspringgreen']\n",
    "text_vars = ['name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'host_about']\n",
    "\n",
    "def plotter_helper(listings_df, text_vars_list, engineered_feature_name, colour_list, plot_kind='bar'):\n",
    "    rows = int(len(text_vars_list)/3)\n",
    "    if len(text_vars_list)%3 > 0:\n",
    "        rows += 1\n",
    "    columns = 3\n",
    "    fig, axes = plt.subplots(rows, columns)\n",
    "    for index, text_var in enumerate(text_vars_list):\n",
    "        text_var_df = booking_rate_filter(None, listings_df, f'{text_var}_{engineered_feature_name}', average_months=True, trim_proportion=0, joined_df_only=False).reset_index()\n",
    "        ax = text_var_df.plot(f'{text_var}_{engineered_feature_name}', 'average_monthly_booking_rate', kind=plot_kind, ax=axes[int(index/columns), int(index%columns)], color=colour_list[index%len(colour_list)])\n",
    "        ax.set_title(f'{text_var}_{engineered_feature_name}')\n",
    "        ax.set_xlabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Joint plots (Monthly Booking Rate and Sentiment Score) for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_helper(four_years_host_listings, text_vars, 'sentiment', colour_list=colours, plot_kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "The sentiment score joint plots for all selected text features are observed to share a highly similiar pattern with maximum average monthly booking rate being situated either at negative sentiment scores, at relatively high sentiment scores. The only exception to this is for the space textual feature where average monthly booking rate peaks at sentiment score = 1. It is evident that the magnitude of sentiment score of for textual featuress have an impact on the booking rate for a listing. <br>\n",
    "\n",
    "However, the charts above are not independent of one another as a host who tend to write highly positive text for a listing's summary is highly likely to write a highly positive text for its description as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Joint plots (Monthly Booking Rate and Number of Adjectives) for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_helper(four_years_host_listings, text_vars, 'adj', colour_list=colours, plot_kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "By analysing all the charts simultaneously, it is evident that majority of listings fall within the same cluster of average monthly booking rates, possibily an indication that the number of adjectives in textual features do not have any impact on the booking rates. <br>\n",
    "\n",
    "However, there are outliers observed in this chart with starkingly contrasting average monthly booking rates. This can be observed from the notes_adj chart (row 2 column 3) as the highest average monthly booking rate (~0.8) listing have the second highest number of adjectives but the listing with the highest number of adjectives can be seen to have an extremely low average booking rate (~0.1). Therefore, the number of adjectives is likely to have no true on booking rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Joint plots (Monthly Booking Rate and Number of Pronouns) for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_helper(four_years_host_listings, text_vars, 'pron', colour_list=colours[4:], plot_kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "It can be seen that all selected textual features have very few pronouns in their respective text values, with the max being 3 pronouns as seen in the notes_pron visualisation. Hence, it is clear that hosts of this group tend not to heavily document their listing from a first-person perspective. In addition, no common signal can be observed from the charts as the peak nor lowest average monthly booking rate does not appear to be similiarly centralised at a location in the various plots.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Joint plots (Monthly Booking Rate and Number of [FACs](https://spacy.io/api/annotation#section-named-entities)) for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_helper(four_years_host_listings, text_vars, 'fac', colour_list=colours[3:-1], plot_kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "FAC as a named entity tag represents identified Buildings, airports, highways, bridges, etc and from glancing at the visualisations, no correlation can be observed between the number of named entities of this nature and the average monthly booking rates as there is no observable common trend between the plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation: Joint plots (Monthly Booking Rate and Number of [GPEs](https://spacy.io/api/annotation#section-named-entities)) for hosts with at least 4 years but less than 5 years hosting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_helper(four_years_host_listings, text_vars, 'gpe', colour_list=colours, plot_kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "GPE is a named entity tag that represents countries, cities and states.<br>\n",
    "\n",
    "Similiarly to previous set of joint plots (Monthly Booking Rate and Number of FACs), there is no observable correlation as evident from comparing summary_gpe plot and neighborhood_overview_gpe plot as their fitted trend line have contrasting gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying poor performing listings and well performing listings and visualising features distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_years_host_listings_filtered = four_years_host_listings[['listing_id', 'year', 'month', 'booking_rate_quartile']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying poor performing listings\n",
    "Deemed as listings with all monthly booking rate quartile being below or equal median (q1 or q2)<br>\n",
    "There are 13 months in the dataframe, 2016 January to 2017 January inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_half_df = four_years_host_listings_filtered[four_years_host_listings_filtered['booking_rate_quartile'] <= 2]\n",
    "lower_half_months_count = lower_half_df.groupby(['listing_id']).size()\n",
    "poor_performing_listings = lower_half_months_count[lower_half_months_count == 13].index\n",
    "poor_performing_four_years_listings = four_years_host_listings[four_years_host_listings['listing_id'].isin(poor_performing_listings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_performing_four_years_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying well performing listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_half_df = four_years_host_listings_filtered[four_years_host_listings_filtered['booking_rate_quartile'] > 2]\n",
    "upper_half_months_count = upper_half_df.groupby(['listing_id']).size()\n",
    "well_performing_listings = upper_half_months_count[upper_half_months_count == 13].index\n",
    "well_performing_four_years_listings = four_years_host_listings[four_years_host_listings['listing_id'].isin(well_performing_listings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_performing_four_years_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising features of poor performing listings and well performing listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "list_of_features = {'numeric': ['booking_rate', 'price', 'number_of_reviews', 'review_scores_rating', 'summary_sentiment'], \n",
    "                    'categorical': ['cancellation_policy', 'host_is_superhost', 'property_type']}\n",
    "num_rows = len(list_of_features['numeric']) + len(list_of_features['categorical'])\n",
    "num_cols = 2\n",
    "fig_height = 8.0*num_rows\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(25.0, fig_height))\n",
    "fig.tight_layout(pad=0.5, h_pad=5.0, w_pad=5.0)\n",
    "fig.subplots_adjust(top=0.965)\n",
    "fig.suptitle('Side-by-side comparison of features (Poor performing listings vs Well performing listings)', fontsize=25)\n",
    "colours = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:grey', 'tab:olive', 'tab:cyan', 'gold', 'mediumspringgreen']\n",
    "index = 0\n",
    "\n",
    "for feature_type, feature_list in list_of_features.items():\n",
    "    if feature_type == 'numeric':\n",
    "        for feature in feature_list:\n",
    "            sns.distplot(poor_performing_four_years_listings[poor_performing_four_years_listings[feature].notnull()][feature], \n",
    "                         ax=axes[index, 0], color=colours[index], axlabel=feature, kde=True).set_title(f'Distribution plot of numerical feature for poor performing listings: {feature}', fontsize=18)\n",
    "            sns.distplot(well_performing_four_years_listings[well_performing_four_years_listings[feature].notnull()][feature], \n",
    "                         ax=axes[index, 1], color=colours[index], axlabel=feature, kde=True).set_title(f'Distribution plot of numerical feature for well performing listings: {feature}', fontsize=18)\n",
    "            index += 1\n",
    "            \n",
    "    else:\n",
    "        for feature in feature_list:\n",
    "            ax1 = poor_performing_four_years_listings[feature].value_counts(normalize=True).plot(kind='barh', ax=axes[index, 0], color=colours[index], fontsize=20)\n",
    "            ax1.set_title(f'Normalised bar plot for {feature}', fontsize=20)\n",
    "            ax1.set_xlabel(\"\")\n",
    "            ax2 = well_performing_four_years_listings[feature].value_counts(normalize=True).plot(kind='barh', ax=axes[index, 1], color=colours[index], fontsize=20)\n",
    "            ax2.set_title(f'Normalised bar plot for {feature}', fontsize=20)\n",
    "            ax2.set_xlabel(\"\")\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:<br>\n",
    "\n",
    "From the pair plots, there are a few interesting observations.<br>\n",
    "\n",
    "(Orange Plot)\n",
    "Firstly, the spread of poor performing listings' prices is much larger than the spread of well performing listings prices with a range of \\\\$1000 compared to \\\\$350. This is empirical evidence that more affordable listings are booked way more often than those that are pricey and that there is a price threshold of $350 per night to Airbnb users for this group of listings.<br>\n",
    "\n",
    "(Red Plot)\n",
    "Secondly, the range of review score ratings for the two subgroups is significantly different as well as the poorer performing listing group has a range of approximately 30 whereas the better performing list group has a range of approxiamtely 20. The better performing listing group has a greater proportion of ratings clustered at 100 than the poorer performing listing group.<br>\n",
    "\n",
    "(Brown Plot)\n",
    "Thirdly, the cancellation policy plots show little noticable difference upon first glance. However, it can be noted that the proportion of strict cancellation policy for better preforming listings is lower than its worse performing counterparts with an estimated proportion of 0.55 and 0.65 respectively. The difference in proportion is accounted for in the difference in proportion between flexible cancellation policies as the better performing listings subgroup has estimated proportion of 0.28 compared with 0.19 of the poor performing listings subgroup.\n",
    "\n",
    "(Grey Plot)\n",
    "Lastly, the property type plots show that the well performing listings group only consist of 2 unique types of property with apartment and house being its unique property types. The poor performing listings group have a larger range of unique property types with a small minority being properties such as boats, lofts, townhouses and condominiums. It is highly indicative that demand for apartment and house is higher than other forms of property types for listings belonging to this group of hosts (hosts with 4-5 years Airbnb experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Classification: modeling listing month's booking rate quartile performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features, encode categorical features into numerical values and standardise numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by months and average values first\n",
    "all_data = listings_booking_rate_quartile.copy(deep=True)\n",
    "aggregated_months = listings_booking_rate_quartile.groupby(['listing_id', 'month']).mean().drop('year', axis=1).reset_index()\n",
    "all_data.update(aggregated_months)\n",
    "\n",
    "# deal with different types of features\n",
    "binary_features = [f'{feature_name}_exist' for feature_name in convert_binary_features_url] + [f'{feature_name}_1' for feature_name in convert_binary_features_tf]\n",
    "\n",
    "categorical_features = ['month', 'neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'cancellation_policy']\n",
    "\n",
    "text_vars_ = [f'{text_var}_' for text_var in ['name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'host_about']]\n",
    "engineered_text_features = []\n",
    "for column in listings_booking_rate_quartile.columns:\n",
    "    for text_var_ in text_vars_:\n",
    "        if text_var_ in column:\n",
    "            engineered_text_features.append(column)\n",
    "\n",
    "numerical_features = ['price', 'security_deposit', 'cleaning_fee', 'accommodates', 'bathrooms', \n",
    "                      'bedrooms', 'beds', 'extra_people', 'number_of_reviews', 'review_scores_rating',\n",
    "                      'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                      'review_scores_communication', 'review_scores_location', 'review_scores_value',\n",
    "                      'calculated_host_listings_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View and check all features' types first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[binary_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[binary_features].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[categorical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[categorical_features].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[engineered_text_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[engineered_text_features].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[numerical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[numerical_features].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values amongst all features, all features has been accounted for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function taken from https://stackoverflow.com/questions/41031767/getting-feature-names-after-one-hot-encoding\n",
    "def one_hot(categorical_features_df, categorical_features_list):\n",
    "    enc_model = OneHotEncoder(sparse=False)\n",
    "    X = enc_model.fit_transform(categorical_features_df[categorical_features_list])\n",
    "    uniq_vals = categorical_features_df[categorical_features_list].apply(lambda x: x.value_counts()).unstack()\n",
    "    uniq_vals = uniq_vals[~uniq_vals.isnull()]\n",
    "    enc_cols = list(uniq_vals.index.map('{0[0]}_{0[1]}'.format))\n",
    "    enc_df = pd.DataFrame(X, columns=enc_cols, index=categorical_features_df.index, dtype=float)\n",
    "    return enc_df, enc_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features_df, ohe_features = one_hot(all_data, categorical_features)\n",
    "ohe_all_data = pd.concat([all_data, ohe_features_df], axis=1, join_axes=[all_data.index])\n",
    "ohe_all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_standardised_data = ohe_all_data.copy(deep=True)\n",
    "scaler = StandardScaler()\n",
    "ohe_standardised_data[engineered_text_features+numerical_features] = scaler.fit_transform(ohe_standardised_data[engineered_text_features+numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using Lasso Regression Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_select_X = ohe_standardised_data[binary_features + ohe_features + engineered_text_features + numerical_features]\n",
    "feature_select_y = ohe_standardised_data['booking_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_clf = LassoCV(cv=10)\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(lasso_clf, threshold=None)\n",
    "sfm.fit(feature_select_X, feature_select_y)\n",
    "\n",
    "n_features = sfm.transform(feature_select_X).shape[1]\n",
    "features_indices = [index for index, boolean in enumerate(sfm.get_support()) if boolean == True]\n",
    "\n",
    "selected_features_X = feature_select_X.iloc[:, features_indices]\n",
    "booking_rate_quartile_y = ohe_standardised_data['booking_rate_quartile'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save independent and dependent features for easier access later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_X.to_csv('selected_features_X.csv', index=False)\n",
    "pd.DataFrame(booking_rate_quartile_y).to_csv('booking_rate_quartile_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    select_features_X.head()\n",
    "except NameError:\n",
    "    selected_features_X = pd.read_csv('selected_features_X.csv')\n",
    "    \n",
    "try:\n",
    "    booking_rate_quartile_y.head()\n",
    "except NameError:\n",
    "    booking_rate_quartile_y = pd.read_csv('booking_rate_quartile_y.csv')['booking_rate_quartile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(selected_features_X)} rows in total for X')\n",
    "print(f'There are {len(booking_rate_quartile_y)} rows in total for y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(selected_features_X, booking_rate_quartile_y, train_size=0.9, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train , y_val = train_test_split(X_train_val, y_train_val, train_size=0.89, random_state=RANDOM_SEED)\n",
    "\n",
    "target_names = ['Q1', 'Q2', 'Q3', 'Q4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform simple minority oversampling technique to deal with class imbalance (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "sm = SMOTE(random_state=RANDOM_SEED, sampling_strategy='auto', k_neighbors=5, n_jobs=-1)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_resampled).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = {'original set': [X_train, y_train], 'oversampled set': [X_resampled, y_resampled]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_type, training_set in training_sets.items():\n",
    "    lr_clf = LogisticRegression(random_state=RANDOM_SEED, multi_class='ovr', max_iter =1000, n_jobs=-1, class_weight='balanced').fit(*training_set)\n",
    "    y_pred = lr_clf.predict(X_val)\n",
    "    print(f'Logistic Regression model trained on {set_type}')\n",
    "    print(f'Accuracy score: {accuracy_score(y_val, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_val, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_val, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_type, training_set in training_sets.items():\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=10, weights='uniform', leaf_size=30, p=2, n_jobs=-1).fit(*training_set)\n",
    "    y_pred = knn_clf.predict(X_val)\n",
    "    print(f'K-Nearest Neighbours model trained on {set_type}')\n",
    "    print(f'Accuracy score: {accuracy_score(y_val, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_val, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_val, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 10, 'eta': 0.1, 'silent': 1, 'subsample': 0.8, 'lambda': 1.0, 'alpha': 0.0, \n",
    "         'objective': 'multi:softmax', 'eval_metric': 'merror', 'num_class': 4}\n",
    "dval = xgb.DMatrix(X_val.as_matrix(), label=y_val.as_matrix()-1)\n",
    "evallist = [(dval, 'validation')]\n",
    "num_round = 100\n",
    "\n",
    "xgb_training_sets = {'original set': xgb.DMatrix(X_train.as_matrix(), label=y_train.as_matrix()-1), \n",
    "                     'oversampled set': xgb.DMatrix(X_resampled, label=y_resampled-1)}\n",
    "\n",
    "for set_type, xgb_training_set in xgb_training_sets.items():\n",
    "    bst = xgb.train(param, xgb_training_set, num_round, evallist, verbose_eval=False)\n",
    "    y_pred = bst.predict(xgb.DMatrix(X_val.as_matrix())) + 1\n",
    "    print(f'XGBoost model trained on {set_type}')\n",
    "    print(f'Accuracy score: {accuracy_score(y_val, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_val, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_val, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_type, training_set in training_sets.items():\n",
    "    svm_clf = SVC(C=1.0, gamma='scale', decision_function_shape='ovr', random_state=RANDOM_SEED, max_iter=-1).fit(*training_set) \n",
    "    y_pred = svm_clf.predict(X_val)\n",
    "    print(f'Support Vector Machine model trained on {set_type}')\n",
    "    print(f'Accuracy score: {accuracy_score(y_val, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_val, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_val, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running grid search on KNN and XGBoost to obtain optimal hyperparameters, models chosen over SVM due to computational constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_params(X, y, nfolds, param_grid, model_instance):\n",
    "    grid_search = GridSearchCV(model_instance, param_grid, cv=nfolds, n_jobs=-1, scoring='f1_weighted', verbose=2, return_train_score=False)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search: K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {'n_neighbors': np.arange(5, 20, 1), 'weights': ['uniform', 'distance'], 'leaf_size': np.arange(20, 42, 1)}\n",
    "knn_params = grid_search_params(X_train_val, y_train_val, nfolds, knn_param_grid, KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters obtained from grid search\n",
    "knn_params = {'leaf_size': 20, 'n_neighbors': 5, 'weights': 'distance'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_type, training_set in training_sets.items():\n",
    "    knn_clf = KNeighborsClassifier(**knn_params).fit(*training_set)\n",
    "    y_pred = knn_clf.predict(X_val)\n",
    "    print(f'K-Nearest Neighbours model trained on {set_type} with grid search parameters, testing on hold-out validation set')\n",
    "    print(f'Accuracy score: {accuracy_score(y_val, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_val, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_val, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_type, training_set in training_sets.items():\n",
    "    knn_clf = KNeighborsClassifier(**knn_params).fit(*training_set)\n",
    "    y_pred = knn_clf.predict(X_test)\n",
    "    print(f'K-Nearest Neighbours model trained on {set_type} with grid search parameters, testing on hold-out test set')\n",
    "    print(f'Accuracy score: {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_test, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_test, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {'max_depth': np.arange(6,9), 'eta': np.linspace(0.02, 0.1, num=5), \n",
    "                  'subsample': np.linspace(0.6, 1.0, num=3), 'colsample_bytree': np.linspace(0.6, 1.0, num=3), \n",
    "                  'min_child_weight': np.arange(1, 3), 'objective': ['multi:softmax'], 'eval_metric': ['merror'], \n",
    "                  'num_class': [4], 'n_estimators': [100], 'seed': [RANDOM_SEED], 'silent': [1]}\n",
    "xgb_params = grid_search_params(X_train_val, y_train_val, nfolds, xgb_param_grid, xgb.XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'colsample_bytree': 1.0, 'eta': 0.02, 'eval_metric': 'merror',\n",
    "              'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 100,\n",
    " 'num_class': 4,\n",
    " 'objective': 'multi:softmax',\n",
    " 'seed': 15,\n",
    " 'silent': 1,\n",
    " 'subsample': 0.6}\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 500\n",
    "\n",
    "xgb_training_sets = {'original set': xgb.DMatrix(X_train.as_matrix(), label=y_train.as_matrix()-1), \n",
    "                     'oversampled set': xgb.DMatrix(X_resampled, label=y_resampled-1)}\n",
    "\n",
    "for set_type, xgb_training_set in xgb_training_sets.items():\n",
    "    bst = xgb.train(xgb_params, xgb_training_set, num_round, verbose_eval=False)\n",
    "    y_pred = bst.predict(xgb.DMatrix(X_test.as_matrix())) + 1\n",
    "    print(f'XGBoost model trained on {set_type} with grid search parameters, testing on hold-out test set')\n",
    "    print(f'Accuracy score: {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "    print(f'F1-score: {f1_score(y_test, y_pred, average=\"weighted\")*100:.2f}%')\n",
    "    print(f'{classification_report(y_test, y_pred, target_names=target_names)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
